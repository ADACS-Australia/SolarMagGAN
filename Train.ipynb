{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "import numpy as np\n",
    "import os, glob, time\n",
    "from random import shuffle\n",
    "from imageio import imread\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, ZeroPadding2D, BatchNormalization, Input, Dropout\n",
    "from keras.layers import Conv2DTranspose, Activation, Cropping2D\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x11233a470>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['KERAS_BACKEND']        = 'tensorflow'\n",
    "os.environ['CUDA_DEVICE_ORDER']    = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "CH_AXIS = -1\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DN_CONV(f, *a, **k):\n",
    "    return Conv2D(f, kernel_initializer = CONV_INIT, *a, **k)\n",
    "\n",
    "def UP_CONV(f, *a, **k):\n",
    "    return Conv2DTranspose(f, kernel_initializer = CONV_INIT, *a, **k)\n",
    "\n",
    "def BATNORM():\n",
    "    return BatchNormalization(momentum=0.9, axis=CH_AXIS, epsilon=1.01e-5, gamma_initializer = GAMMA_INIT)\n",
    "\n",
    "def LEAKY_RELU(alpha):\n",
    "    return LeakyReLU(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BASIC_D(ISIZE, NC_IN, NC_OUT, MAX_LAYERS):\n",
    "    INPUT_A, INPUT_B = Input(shape = (ISIZE, ISIZE, NC_IN)), Input(shape = (ISIZE, ISIZE, NC_OUT))\n",
    "    INPUT = Concatenate(axis=CH_AXIS) ([INPUT_A, INPUT_B])\n",
    "\n",
    "    if MAX_LAYERS == 0 :\n",
    "        N_FEATURE = 1        \n",
    "        L = DN_CONV(N_FEATURE, kernel_size = 1, padding = 'same', activation = 'sigmoid') (INPUT)\n",
    "\n",
    "    else :\n",
    "        N_FEATURE = 64\n",
    "        L = DN_CONV(N_FEATURE, kernel_size = 4, strides = 2, padding = \"same\") (INPUT)\n",
    "        L = LEAKY_RELU(0.2) (L)\n",
    "\n",
    "        for i in range(1, MAX_LAYERS):\n",
    "            N_FEATURE *= 2\n",
    "            L = DN_CONV(N_FEATURE, kernel_size = 4, strides = 2, padding = \"same\") (L)\n",
    "            L = BATNORM() (L, training = 1)\n",
    "            L = LEAKY_RELU(0.2) (L)\n",
    "            \n",
    "        N_FEATURE *= 2\n",
    "        L = ZeroPadding2D(1) (L)\n",
    "        L = DN_CONV(N_FEATURE, kernel_size = 4, padding = \"valid\") (L)\n",
    "        L = BATNORM() (L, training = 1)\n",
    "        L = LEAKY_RELU(0.2) (L)\n",
    "\n",
    "        N_FEATURE = 1\n",
    "        L = ZeroPadding2D(1) (L)\n",
    "        L = DN_CONV(N_FEATURE, kernel_size = 4, padding = \"valid\", activation = 'sigmoid') (L)\n",
    "    \n",
    "    return Model(inputs = [INPUT_A, INPUT_B], outputs = L)    \n",
    "\n",
    "def UNET_G(ISIZE, NC_IN, NC_OUT, FIXED_INPUT_SIZE = True):    \n",
    "    MAX_N_FEATURE = 64 * 8\n",
    "    def BLOCK(X, S, NF_IN, USE_BATNORM = True, NF_OUT = None, NF_NEXT = None):\n",
    "        assert S >= 2 and S%2 == 0\n",
    "        if NF_NEXT is None:\n",
    "            NF_NEXT = min(NF_IN*2, MAX_N_FEATURE)\n",
    "        if NF_OUT is None:\n",
    "            NF_OUT = NF_IN\n",
    "        X = DN_CONV(NF_NEXT, kernel_size = 4, strides = 2, use_bias = (not (USE_BATNORM and S > 2)), padding = \"same\") (X)\n",
    "        if S > 2:\n",
    "            if USE_BATNORM:\n",
    "                X = BATNORM() (X, training = 1)\n",
    "            X2 = LEAKY_RELU(0.2) (X)\n",
    "            X2 = BLOCK(X2, S//2, NF_NEXT)\n",
    "            X = Concatenate(axis = CH_AXIS) ([X, X2])            \n",
    "        X = Activation(\"relu\") (X)\n",
    "        X = UP_CONV(NF_OUT, kernel_size = 4, strides = 2, use_bias = not USE_BATNORM) (X)\n",
    "        X = Cropping2D(1) (X)\n",
    "        if USE_BATNORM:\n",
    "            X = BATNORM() (X, training = 1)\n",
    "        if S <= 8:\n",
    "            X = Dropout(0.5) (X, training = 1)\n",
    "        return X\n",
    "    \n",
    "    S = ISIZE if FIXED_INPUT_SIZE else None\n",
    "    X = INPUT = Input(shape = (S, S, NC_IN))\n",
    "    X = BLOCK(X, ISIZE, NC_IN, False, NF_OUT = NC_OUT, NF_NEXT = 64)\n",
    "    X = Activation('tanh') (X)\n",
    "\n",
    "    return Model(inputs = INPUT, outputs = [X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LOAD_DATA(FILE_PATTERN):\n",
    "    return glob.glob(FILE_PATTERN)\n",
    "\n",
    "def READ_IMAGE(FN, NC_IN, NC_OUT):\n",
    "    IMG_A = imread(FN[0])\n",
    "    IMG_B = imread(FN[1])\n",
    "    X, Y = np.random.randint(31), np.random.randint(31)\n",
    "\n",
    "    if NC_IN != 1 :\n",
    "        IMG_A = np.pad(IMG_A, ((15, 15), (15, 15), (0, 0)), 'constant')\n",
    "        IMG_A = IMG_A[X:X + 1024, Y:Y + 1024,:] / 255.0 * 2 - 1\n",
    "    else :\n",
    "        IMG_A = np.pad(IMG_A, 15, 'constant')\n",
    "        IMG_A = IMG_A[X:X + 1024, Y:Y + 1024] / 255.0 * 2 - 1\n",
    "\n",
    "    if NC_OUT != 1 :\n",
    "        IMG_B = np.pad(IMG_B, ((15, 15), (15, 15), (0, 0)), 'constant')\n",
    "        IMG_B = IMG_B[X:X + 1024, Y:Y + 1024,:] / 255.0 * 2 - 1\n",
    "    else :\n",
    "        IMG_B = np.pad(IMG_B, 15, 'constant')\n",
    "        IMG_B = IMG_B[X:X + 1024, Y:Y + 1024] / 255.0 * 2 - 1\n",
    "\n",
    "    return IMG_A, IMG_B\n",
    "\n",
    "def MINI_BATCH(DATA_AB, BATCH_SIZE, NC_IN, NC_OUT):\n",
    "    LENGTH = len(DATA_AB)\n",
    "    EPOCH = I = 0\n",
    "    TMP_SIZE = None\n",
    "    while True:\n",
    "        SIZE = TMP_SIZE if TMP_SIZE else BATCH_SIZE\n",
    "        if I + SIZE > LENGTH:\n",
    "            shuffle(DATA_AB)\n",
    "            I = 0\n",
    "            EPOCH += 1\n",
    "        DATA_A = []\n",
    "        DATA_B = []\n",
    "        for J in range(I, I + SIZE):\n",
    "            IMG_A,IMG_B = READ_IMAGE(DATA_AB[J], NC_IN, NC_OUT)\n",
    "            DATA_A.append(IMG_A)\n",
    "            DATA_B.append(IMG_B)\n",
    "        DATA_A = np.float32(DATA_A)\n",
    "        DATA_B = np.float32(DATA_B)\n",
    "        I += SIZE\n",
    "        TMP_SIZE = yield EPOCH, DATA_A, DATA_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './MODELS/ORIGINAL_MxLr3/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-18c6e9a227dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mMODEL_PATH_MAIN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./MODELS/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTRIAL_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH_MAIN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH_MAIN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mMODEL_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_PATH_MAIN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMODE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './MODELS/ORIGINAL_MxLr3/'"
     ]
    }
   ],
   "source": [
    "#%% Hyper parameters\n",
    "\n",
    "NITERS = 500000\n",
    "DISPLAY_ITERS = 5000\n",
    "\n",
    "INPUT_DATA = 'AIA0304'\n",
    "OUTPUT_DATA = 'HMI0100'\n",
    "\n",
    "ISIZE = 1024\n",
    "NC_IN = 1\n",
    "NC_OUT = 1\n",
    "BATCH_SIZE = 1\n",
    "MAX_LAYERS = 3 #1 for 16, 2 for 34, 3 for 70, 4 for 142, and 5 for 286\n",
    "\n",
    "TRIAL_NAME = 'ORIGINAL_MxLr' + str(MAX_LAYERS)\n",
    "\n",
    "#%%\n",
    "\n",
    "MODE = INPUT_DATA + '_to_' + OUTPUT_DATA\n",
    "\n",
    "IMAGE_PATH_INPUT = './DATA/TRAIN/'+INPUT_DATA+'/*.png'\n",
    "IMAGE_PATH_OUTPUT = './DATA/TRAIN/'+OUTPUT_DATA+'/*.png'\n",
    "\n",
    "MODEL_PATH_MAIN = './MODELS/' + TRIAL_NAME + '/'\n",
    "os.mkdir(MODEL_PATH_MAIN) if not os.path.exists(MODEL_PATH_MAIN) else None\n",
    "\n",
    "MODEL_PATH = MODEL_PATH_MAIN + MODE + '/'\n",
    "os.mkdir(MODEL_PATH) if not os.path.exists(MODEL_PATH) else None\n",
    "\n",
    "#%%\n",
    "\n",
    "CONV_INIT = RandomNormal(0, 0.02)\n",
    "GAMMA_INIT = RandomNormal(1., 0.02)\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "NET_D = BASIC_D(ISIZE, NC_IN, NC_OUT, MAX_LAYERS)\n",
    "NET_G = UNET_G (ISIZE, NC_IN, NC_OUT)\n",
    "\n",
    "REAL_A = NET_G.input\n",
    "FAKE_B = NET_G.output\n",
    "REAL_B = NET_D.inputs[1]\n",
    "\n",
    "OUTPUT_D_REAL = NET_D([REAL_A, REAL_B])\n",
    "OUTPUT_D_FAKE = NET_D([REAL_A, FAKE_B])\n",
    "\n",
    "LOSS_FN = lambda OUTPUT, TARGET : -K.mean(K.log(OUTPUT+1e-12)*TARGET+K.log(1-OUTPUT+1e-12)*(1-TARGET))\n",
    "\n",
    "LOSS_D_REAL = LOSS_FN(OUTPUT_D_REAL, K.ones_like(OUTPUT_D_REAL))\n",
    "LOSS_D_FAKE = LOSS_FN(OUTPUT_D_FAKE, K.zeros_like(OUTPUT_D_FAKE))\n",
    "LOSS_G_FAKE = LOSS_FN(OUTPUT_D_FAKE, K.ones_like(OUTPUT_D_FAKE))\n",
    "\n",
    "LOSS_L = K.mean(K.abs(FAKE_B-REAL_B))\n",
    "\n",
    "\n",
    "LOSS_D = LOSS_D_REAL + LOSS_D_FAKE\n",
    "TRAINING_UPDATES_D = Adam(lr = 2e-4, beta_1 = 0.5).get_updates(NET_D.trainable_weights, [], LOSS_D)\n",
    "NET_D_TRAIN = K.function([REAL_A, REAL_B], [LOSS_D/2.0], TRAINING_UPDATES_D)\n",
    "\n",
    "LOSS_G = LOSS_G_FAKE + 100 * LOSS_L\n",
    "TRAINING_UPDATES_G = Adam(lr = 2e-4, beta_1 = 0.5).get_updates(NET_G.trainable_weights, [], LOSS_G)\n",
    "NET_G_TRAIN = K.function([REAL_A, REAL_B], [LOSS_G_FAKE, LOSS_L], TRAINING_UPDATES_G)\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "LIST_INPUT = LOAD_DATA(IMAGE_PATH_INPUT)\n",
    "LIST_OUTPUT = LOAD_DATA(IMAGE_PATH_OUTPUT)\n",
    "assert len(LIST_INPUT) == len(LIST_OUTPUT)\n",
    "LIST_TOTAL = list(zip(sorted(LIST_INPUT), sorted(LIST_OUTPUT)))\n",
    "TRAIN_BATCH = MINI_BATCH(LIST_TOTAL, BATCH_SIZE, NC_IN, NC_OUT)\n",
    "\n",
    "#%%\n",
    "\n",
    "T0 = T1 = time.time()\n",
    "GEN_ITERS = 0\n",
    "ERR_L = 0\n",
    "EPOCH = 0\n",
    "ERR_G = 0\n",
    "ERR_L_SUM = 0\n",
    "ERR_G_SUM = 0\n",
    "ERR_D_SUM = 0\n",
    "\n",
    "while GEN_ITERS <= NITERS :\n",
    "    EPOCH, TRAIN_A, TRAIN_B = next(TRAIN_BATCH)\n",
    "    TRAIN_A = TRAIN_A.reshape((BATCH_SIZE, ISIZE, ISIZE, NC_IN))\n",
    "    TRAIN_B = TRAIN_B.reshape((BATCH_SIZE, ISIZE, ISIZE, NC_OUT))\n",
    "\n",
    "    ERR_D,  = NET_D_TRAIN([TRAIN_A, TRAIN_B])\n",
    "    ERR_D_SUM += ERR_D\n",
    "\n",
    "    ERR_G, ERR_L = NET_G_TRAIN([TRAIN_A, TRAIN_B])\n",
    "    ERR_G_SUM += ERR_G\n",
    "    ERR_L_SUM += ERR_L\n",
    "\n",
    "    GEN_ITERS += 1\n",
    "\n",
    "    if GEN_ITERS%DISPLAY_ITERS==0:\n",
    "        print('[%d][%d/%d] LOSS_D: %5.3f LOSS_G: %5.3f LOSS_L: %5.3f T: %dsec/%dits, Total T: %d'\n",
    "        % (EPOCH, GEN_ITERS, NITERS, ERR_D_SUM/DISPLAY_ITERS, ERR_G_SUM/DISPLAY_ITERS, ERR_L_SUM/DISPLAY_ITERS, time.time()-T1, DISPLAY_ITERS, time.time()-T0))\n",
    "\n",
    "        ERR_L_SUM = 0\n",
    "        ERR_G_SUM = 0\n",
    "        ERR_D_SUM = 0\n",
    "        DST_MODEL = MODEL_PATH+MODE+'_ITER'+'%07d'%GEN_ITERS+'.h5'\n",
    "        NET_G.save(DST_MODEL)\n",
    "        T1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
